---
title: 'Psych 251 PS4: Simulation'
author: "Mike Frank"
date: "2018"
output: 
  html_document:
    toc: true
---

This is problem set #4, in which we want you to integrate your knowledge of data wrangling with some basic simulation skills. It's a short problem set to help you get your feet wet in testing statistical concepts through "making up data" rather than consulting a textbook or doing math. 

For ease of reading, please separate your answers from our text by marking our text with the `>` character (indicating quotes). 

```{r}
library(tidyverse)
```

Let's start by convincing ourselves that t-tests have the appropriate false positive rate. Run 10,000 t-tests with standard, normally-distributed data from a made up 30-person, single-measurement experiment (the command for sampling from a normal distribution is `rnorm`). What's the proportion of "significant" results ($p < .05$) that you see?

First do this using a `for` loop.

```{r}
```

Next, do this using the `replicate` function:

```{r}
```

Ok, that was a bit boring. Let's try something more interesting - let's implement a p-value sniffing simulation, in the style of Simons, Nelson, & Simonsohn (2011).

Consider this scenario: you have done an experiment, again with 30 participants (one observation each, just for simplicity). The question is whether their performance is above chance. You aren't going to check the p-value every trial, but let's say you run 30 - then if the p-value is within the range p < .25 and p > .05, you optionally run 30 more and add those data, then test again. But if the original p value is < .05, you call it a day, and if the original is > .25, you also stop.  

First, write a function that implements this sampling regime.

```{r}
double.sample <- function () {
}
```

Now call this function 10k times and find out what happens. 

```{r}
```

Is there an inflation of false positives? How bad is it?

> ANSWER

Now modify this code so that you can investigate this "double the sample" rule in a bit more depth. Let's see what happens when you double the sample ANY time p > .05 (not just when p < .25), or when you do it only if p < .5 or < .75. How do these choices affect the false positive rate?

HINT: Try to do this by making the function `double.sample` take the upper p value as an argument, so that you can pass this through dplyr.

HINT 2: You may need more samples. Find out by looking at how the results change from run to run.

```{r}
```

What do you conclude on the basis of this simulation? How bad is this kind of data-dependent policy?

> ANSWER
